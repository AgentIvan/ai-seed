{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes Classifier with Standard Scaler\n",
    "\n",
    "### Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as se \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report,plot_confusion_matrix \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Filepath of CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath\n",
    "file_path= \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of features which are  required for model training ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_values\n",
    "features= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target feature for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_value\n",
    "target=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fetching\n",
    "\n",
    "Pandas is an open-source, BSD-licensed library providing high-performance, easy-to-use data manipulation and data analysis tools.\n",
    "\n",
    "We will use panda's library to read the CSV file using its storage path.And we use the head function to display the initial row or entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selections\n",
    "\n",
    "It is the process of reducing the number of input variables when developing a predictive model. Used to reduce the number of input variables to both reduce the computational cost of modelling and, in some cases, to improve the performance of the model.\n",
    "\n",
    "We will assign all the required input features to X and target/outcome to Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features]\n",
    "Y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Since the majority of the machine learning models in the Sklearn library doesn't handle string category data and Null value, we have to explicitly remove or replace null values. The below snippet have functions, which removes the null value if any exists. And convert the string classes data in the datasets by encoding them to integer classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NullClearner(df):\n",
    "    if(isinstance(df, pd.Series) and (df.dtype in [\"float64\",\"int64\"])):\n",
    "        df.fillna(df.mean(),inplace=True)\n",
    "        return df\n",
    "    elif(isinstance(df, pd.Series)):\n",
    "        df.fillna(df.mode()[0],inplace=True)\n",
    "        return df\n",
    "    else:return df\n",
    "def EncodeX(df):\n",
    "    return pd.get_dummies(df)\n",
    "def EncodeY(df):\n",
    "    if len(df.unique())<=2:\n",
    "        return df\n",
    "    else:\n",
    "        un_EncodedT=np.sort(pd.unique(df), axis=-1, kind='mergesort')\n",
    "        df=LabelEncoder().fit_transform(df)\n",
    "        EncodedT=[xi for xi in range(len(un_EncodedT))]\n",
    "        print(\"Encoded Target: {} to {}\".format(un_EncodedT,EncodedT))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=X.columns.to_list()\n",
    "for i in x:\n",
    "    X[i]=NullClearner(X[i])  \n",
    "X=EncodeX(X)\n",
    "Y=EncodeY(NullClearner(Y))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Map\n",
    "\n",
    "In order to check the correlation between the features, we will plot a correlation matrix. It is effective in summarizing a large amount of data where the goal is to see patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(18, 18))\n",
    "matrix = np.triu(X.corr())\n",
    "se.heatmap(X.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax, mask=matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution Of Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "se.countplot(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "The train-test split is a procedure for evaluating the performance of an algorithm. The procedure involves taking a dataset and dividing it into two subsets. The first subset is utilized to fit/train the model. The second subset is used for prediction. The main motive is to estimate the performance of the model on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Target Imbalance\n",
    "\n",
    "The challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.\n",
    "\n",
    "One approach to addressing imbalanced datasets is to oversample the minority class. The simplest approach involves duplicating examples in the minority class.We will perform overspampling using imblearn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = RandomOverSampler(random_state=123).fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Gaussian NB is a variant of Naive Bayes that follows Gaussian normal distribution and supports continuous data. An approach to creating a simple model is to assume that the data is described by a Gaussian distribution with no co-variance between features. \n",
    "\n",
    "\n",
    "#### Model Tuning Parameters\n",
    "\n",
    "    1. priors : array-like of shape (n_classes,)\n",
    "> Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
    "\n",
    "    2. var_smoothing : float, default=1e-9\n",
    "> Portion of the largest variance of all features that is added to variances for calculation stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler\n",
    "Standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$z = (x - u) / s$\n",
    "\n",
    "where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.\n",
    "\n",
    "\n",
    "<h4 style=\"color:orange;\">For More Reference :-</h4> \n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model here\n",
    "model = make_pipeline(StandardScaler(),GaussianNB())\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Accuracy\n",
    "score() method return the mean accuracy on the given test data and labels.\n",
    "\n",
    "In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score {:.2f} %\\n\".format(model.score(x_test,y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "A confusion matrix is utilized to understand the performance of the classification model or algorithm in machine learning for a given test set where results are known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model,x_test,y_test,cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report\n",
    "A Classification report is used to measure the quality of predictions from a classification algorithm. How many predictions are True, how many are False.\n",
    "\n",
    "* **where**:\n",
    "    - Precision:- Accuracy of positive predictions.\n",
    "    - Recall:- Fraction of positives that were correctly identified.\n",
    "    - f1-score:-  percent of positive predictions were correct\n",
    "    - support:- Support is the number of actual occurrences of the class in the specified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creator: Jay Shimpi , Github: [Profile](https://github.com/JayShimpi22)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51a9663a131f1b5758c45b97a2d6917c8ae86b33e231c3733631cbc7265cfc89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
