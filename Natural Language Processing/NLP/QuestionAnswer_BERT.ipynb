{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionAnswer_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-1zl5XdYInf"
      },
      "source": [
        "# Question Answer Application using BRET\n",
        "This Code Template  is for Question_Answer_Application using BRET in python \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVq-TuylYRDW"
      },
      "source": [
        "## Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQl0MMrOGIup"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mzZJJII62wG"
      },
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import textwrap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIb7HYgt7G-r"
      },
      "source": [
        "### Initialization\n",
        "Enter the text from which we want to ask the question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ov4qMBm7XKY"
      },
      "source": [
        "Text=\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\" "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WThOUtpYvG-"
      },
      "source": [
        "### Model\n",
        "#BERT\n",
        "The BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. Itâ€™s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n",
        "\n",
        "Refer [API](https://huggingface.co/transformers/model_doc/bert.html) for the parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyg45Fbm9ZD-"
      },
      "source": [
        "As our reference text, I've taken the Abstract of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mnv95sX-U9K"
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSL8qvfRBh77"
      },
      "source": [
        "#steps involved in answer_question Function \n",
        "Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "\n",
        "\n",
        "The number of segment A tokens includes the [SEP] token istelf.\n",
        "\n",
        "Construct the list of 0s and 1s.\n",
        "\n",
        "There should be a segment_id for every input token.\n",
        "\n",
        "Find the tokens with the highest `start` and `end` scores.\n",
        "\n",
        "Get the string versions of the input tokens.\n",
        "\n",
        "Select the remaining answer tokens and join them with whitespace.\n",
        "\n",
        "If it's a subword token, then recombine it with the previous token.\n",
        "\n",
        "Otherwise, add a space then the token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH8NbBlsfxZ_"
      },
      "source": [
        "def answer_question(question, answer_text):\n",
        "    '''\n",
        "    Takes a `question` string and an `answer_text` string (which contains the\n",
        "    answer), and identifies the words within the `answer_text` that are the\n",
        "    answer. Prints them out.\n",
        "    '''\n",
        " \n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "    num_seg_a = sep_index + 1\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "    outputs = model(torch.tensor([input_ids]), \n",
        "    token_type_ids=torch.tensor([segment_ids]), \n",
        "    return_dict=True) \n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer = tokens[answer_start]\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        \n",
        "\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        \n",
        "\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4VPq6FdjxyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79614afa-984e-47fa-a935-928c5453ad69"
      },
      "source": [
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "bert_abstract =Text\n",
        "print(wrapper.fill(bert_abstract))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We introduce a new language representation model called BERT, which stands for\n",
            "Bidirectional Encoder Representations from Transformers. Unlike recent language\n",
            "representation models (Peters et al., 2018a; Radford et al., 2018), BERT is\n",
            "designed to pretrain deep bidirectional representations from unlabeled text by\n",
            "jointly conditioning on both left and right context in all layers. As a result,\n",
            "the pre-trained BERT model can be finetuned with just one additional output\n",
            "layer to create state-of-the-art models for a wide range of tasks, such as\n",
            "question answering and language inference, without substantial taskspecific\n",
            "architecture modifications. BERT is conceptually simple and empirically\n",
            "powerful. It obtains new state-of-the-art results on eleven natural language\n",
            "processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute\n",
            "improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1\n",
            "question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD\n",
            "v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEB654YCknYv"
      },
      "source": [
        "-----------------------------\n",
        "Ask BERT what its name stands for (the answer is in the first sentence of the abstract)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfntqRCBegGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f254d4c7-3e97-4a8f-c383-915bcc2e68eb"
      },
      "source": [
        "question = \"What does the 'B' in BERT stand for?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query has 258 tokens.\n",
            "\n",
            "Answer: \"bidirectional encoder representations from transformers\"\n"
          ]
        }
      ]
    }
  ]
}